---
title: "Text Prognosticator"
author: "Justin Papreck"
date: "January 25, 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Prognosicator is the back-end of the Coursera Capstone Project in collaboration with Swift-Key for the Data Science Specialization through Johns-Hopkins University. The Prognosicator Pipeline uses text data from English language corpora obtained from news sources, blogs, and twitter feeds. Following processing, these corpora are used to make predictions for the next word. All new word predictions have profanities removed, however the program still makes predictions based on profinities in the input phrases. 

## This code was written for optimized use on a laptop computer with 8 GB of available RAM. The trade-offs considered due to the RAM available was the number of chunks each file was broken into, the number of n-grams to which the corpora were broken into, and the largest set of filtered data that can be opened and used after processing and filtration of data. 

## This code broke the raw corpora into 20 smaller corpus files to process n-grams, which were then compiled into 5 data tables. This computer was able to run up to 5-grams before it produced memory allocation errors. Before the final steps, all 5 of the filtered data tables were concatenated and probabilites for predictions were calculated from that final data table. 

# Pipeline
## Preprocessing
    1. Set the directory, path, and open all of the libraries
    2. Open and read the corpora, count the number of lines in each corpus
    3. Split the corpora into different data sets and write to 20 smaller files
## Processing
    4. Read lines from the new split files, then save 4 files each to 5 data tables
    5. Filter each table using the profanity index
    6. Concatenate all of the above tables and save
    7. Process any aggregates, calculate probabilities, and save as text file
## Execute
    8. Source Prognosicator.R


# Preprocessing
## 1. Set the directory, path, and open all of the libraries 
```{r Directory, echo=TRUE}
    working.directory <- "C:/Users/Caroline/Desktop/Coursera-SwiftKey/"
    setwd(working.directory)
    path <- paste0(working.directory, "data/final/en_US/")
    
    suppressMessages(library(quanteda))
    suppressMessages(library(data.table))
    suppressMessages(library(readtext))
    suppressMessages(library(dplyr))
```
   
## 2. Open, Read, Count Lines
    Open the corpora
``` {r Open, echo=TRUE}
    options(warn = -1)
    unzip("Coursera-Swiftkey.zip", exdir = "data", overwrite = FALSE, setTimes = FALSE)
    blog.con <- file(paste0(path, "en_US.blogs.txt"), "rb", encoding = "UTF-8")
    news.con <- file(paste0(path, "en_US.blogs.txt"), "rb", encoding = "UTF-8")
    twitter.con <- file(paste0(path, "en_US.twitter.txt"), "rb", encoding = "UTF-8")
```
    
    Open the number of lines in each corpus
```{r Lines, echo=TRUE}    
    blog.lines <- countLines(paste0(path, "en_US.blogs.txt"))
    news.lines <- countLines(paste0(path, "en_US.news.txt"))
    twitter.lines <- countLines(paste0(path, "en_US.twitter.txt"))
```

## 3. Split the corpora into different data sets and write to 20 smaller files
    Set seed and establish Test and Training Paths
``` {r Split, echo=TRUE}
    set.seed(1117)
    training <- lapply(paste0(path, "training_", 1:20, ".txt"), file, "w")
    testing <- file(paste0(path, "testing.txt"), "w")
```

    Define the Corpus Axe text splitting function, a similar function is used for the input text
``` {r Axe, echo=TRUE}
    corpus.Axe <- function(corpus.con, corpus.lines) {
            training.Number <- rbinom(corpus.lines, 1, 0.9) # Strip 90% of the text for the training files
            Number <- rep_along(training.Number, 1:20) # Split into 20 separate files
            for (i in 1:corpus.lines) {
                Lines <- readLines(corpus.con, n = 1, skipNul = T, encoding = "UTF-8") # n = 1 read one line at a time
                if (training.Number[i]) {
                    writeLines(Lines, con = training[[Number[i]]]) # Write the different files
                } else { 
                    writeLines(Lines, con = testing)} # 10% of the text is reserved for test files
            }
            close(corpus.con)
    }    
```

    Read the corpora and actually split the files using corpus.Axe
```{r Chop, echo=TRUE}
    corpus.Axe(blog.con, blog.lines)
    corpus.Axe(twitter.con, twitter.lines)
    corpus.Axe(news.con, news.lines)
    
    invisible(lapply(training, close))
    close(testing)
    rm(blog.con, blog.lines, news.con, news.lines, twitter.con, twitter.lines)
```

# Processing
    Note: After each chunk was read, R had to be reset. Originally, the code was written to iteratively process all 5 chunks, but R crashed during the reading of the second chunk. After trying to process smaller sets numerous times, due to the constraints of this computer, the optimal size for the training set was 5 chunks of 4 files each. 
    
## 4. Read lines from the new split files, then save 4 files each to 5 data tables 
```{r Chunks, echo=TRUE}
    ## Tokenize Chunk 1 of 5
        training.Path <- paste0(path, "training_", 1:4, ".txt")
        ngram.Table <- data.table()
        
        To <- Sys.time()
        for (file.path in training.Path) {
            print(file.path)
            corpus.Number <- readLines(file.path)
            corpus.Number <- sapply(corpus.Number, tolower, USE.NAMES = F) # removes all capital letters
            corpus.Number <- sapply(corpus.Number, function(x) str_replace_all(x, "[a-z']", " "), USE.NAMES = F)
            # above line replaces all text that is not a lower case letter or an apostrophe [] = NOT
            corpus.Number <- corpus(corpus.Number) # converts type string to corpus
            corpus.Tokens <- tokens(corpus.Number, what = "word", ngrams = 2:5) 
            # above line tokenizes the corpus into n-grams from 2-grams to 5-grams by word
            rm(corpus.Number)
            corpus.Tokens <- unlist(corpus.Tokens, use.names = F) %>% as.data.table()
            # above line converts the tokens data.frame into a data.table, which is less intensive for processing
            names(corpus.Tokens) <- "nGram" # adds a heading called "nGram"
            ngram.Table <- rbind(ngram.Table, corpus.Tokens[, list(count = .N), by = list(nGram)])
            # above line adds rows to the empty table ngram.Table sorted by n-gram, and counts the number of like n-grams
            rm(corpus.Tokens)
            gc()
        }
        print(Sys.time() - To)
        print("One down, 4 to go! If R takes twice as long to process the next chunk, consider restarting R")
        
        save(ngram.Table, file = paste0(path, "ngram.Table1.RData"))
        rm(ngram.Table)

    ## Tokenize Chunk 2 of 5
        training.Path <- paste0(path, "training_", 5:8, ".txt")
        ngram.Table <- data.table()
        
        To <- Sys.time()
        for (file.path in training.Path) {
            print(file.path)
            corpus.Number <- readLines(file.path)
            corpus.Number <- sapply(corpus.Number, tolower, USE.NAMES = F)
            corpus.Number <- sapply(corpus.Number, function(x) str_replace_all(x, "[a-z']", " "), USE.NAMES = F)
            corpus.Number <- corpus(corpus.Number)
            corpus.Tokens <- tokens(corpus.Number, what = "word", ngrams = 2:5)
            rm(corpus.Number)
            corpus.Tokens <- unlist(corpus.Tokens, use.names = F) %>% as.data.table()
            names(corpus.Tokens) <- "nGram"
            ngram.Table <- rbind(ngram.Table, corpus.Tokens[, list(count = .N), by = list(nGram)])
            # above line adds rows from this file to the end of ngram.Table, already containing ngrams from Chunk 1
            rm(corpus.Tokens)
            gc()
        }
        print(Sys.time() - To); print("Two down, Three to go. You may need to reset R")
        
        save(ngram.Table, file = paste0(path, "ngram.Table2.RData"))
        rm(ngram.Table)
        
    ## Tokenize Chunk 3 of 5
        training.Path <- paste0(path, "training_", 9:12, ".txt")
        ngram.Table <- data.table()
        
        To <- Sys.time()
        for (file.path in training.Path) {
            print(file.path)
            corpus.Number <- readLines(file.path)
            corpus.Number <- sapply(corpus.Number, tolower, USE.NAMES = F)
            corpus.Number <- sapply(corpus.Number, function(x) str_replace_all(x, "[a-z']", " "), USE.NAMES = F)
            corpus.Number <- corpus(corpus.Number)
            corpus.Tokens <- tokens(corpus.Number, what = "word", ngrams = 2:5)
            rm(corpus.Number)
            corpus.Tokens <- unlist(corpus.Tokens, use.names = F) %>% as.data.table()
            names(corpus.Tokens) <- "nGram"
            ngram.Table <- rbind(ngram.Table, corpus.Tokens[, list(count = .N), by = list(nGram)])
            rm(corpus.Tokens)
            gc()
        }
        print(Sys.time() - To)
        
        save(ngram.Table, file = paste0(path, "ngram.Table3.RData"))
        rm(ngram.Table)
        
    ## Tokenize Chunk 4 of 5
        training.Path <- paste0(path, "training_", 13:16, ".txt")
        ngram.Table <- data.table()
        
        To <- Sys.time()
        for (file.path in training.Path) {
            print(file.path)
            corpus.Number <- readLines(file.path)
            corpus.Number <- sapply(corpus.Number, tolower, USE.NAMES = F)
            corpus.Number <- sapply(corpus.Number, function(x) str_replace_all(x, "[a-z']", " "), USE.NAMES = F)
            corpus.Number <- corpus(corpus.Number)
            corpus.Tokens <- tokens(corpus.Number, what = "word", ngrams = 2:5)
            rm(corpus.Number)
            corpus.Tokens <- unlist(corpus.Tokens, use.names = F) %>% as.data.table()
            names(corpus.Tokens) <- "nGram"
            ngram.Table <- rbind(ngram.Table, corpus.Tokens[, list(count = .N), by = list(nGram)])
            rm(corpus.Tokens)
            gc()
        }
        print(Sys.time() - To); print("Almost there, one more. You know the drill!")
        
        save(ngram.Table, file = paste0(path, "ngram.Table4.RData"))
        rm(ngram.Table)
        
    ## Tokenize Chunk 5 of 5
        training.Path <- paste0(path, "training_", 17:20, ".txt")
        ngram.Table <- data.table()
        
        To <- Sys.time()
        for (file.path in training.Path) {
            print(file.path)
            corpus.Number <- readLines(file.path)
            corpus.Number <- sapply(corpus.Number, tolower, USE.NAMES = F)
            corpus.Number <- sapply(corpus.Number, function(x) str_replace_all(x, "[a-z']", " "), USE.NAMES = F)
            corpus.Number <- corpus(corpus.Number)
            corpus.Tokens <- tokens(corpus.Number, what = "word", ngrams = 2:5)
            rm(corpus.Number)
            corpus.Tokens <- unlist(corpus.Tokens, use.names = F) %>% as.data.table()
            names(corpus.Tokens) <- "nGram"
            ngram.Table <- rbind(ngram.Table, corpus.Tokens[, list(count = .N), by = list(nGram)])
            rm(corpus.Tokens)
            gc()
        }
        print(Sys.time() - To); print("Finally, that took forever!")
        
        save(ngram.Table, file = paste0(path, "ngram.Table5.RData"))
        rm(ngram.Table)
```

## 5. Filter each table using the profanity index
    Note: nasty.txt is a file that was reduced from the "List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words-master" acquired from Google. The original file was filtering 1700 some profanities, many of which were not words or would have already been filtered in the processing of text. These were removed by hand and saved as 'nasty.txt' in order to improve the efficiency of profanity filtration. Each of the 5 ngram table chunks were filtered separately and saved per memory allocation issues. 
``` {r Filtration, echo=TRUE}
 nasty <- unlist(fread(paste0(working.directory, "nasty.txt"), header = F, fill = T))
        
    # ^ looks for start
    # [] acts as NOT
    # $ looks at end 
    
    ## Filter Chunk 1 of 5
        load(paste0(path, "ngram.Table", 1, ".RData"))
        ngram.Table <- ngram.Table[, list(phrase = sub("_[^_]+$", "", nGram), 
                                          prediction = sub("^([^_]+_)+", "", nGram), count)]
        # Above line looks in the 
        ngram.Table <- ngram.Table[!(prediction %in% nasty)]
        ngram.Table <- ngram.Table[, phrase.count := sum(count), phrase]
        ngram.Table <- ngram.Table[phrase.count > 2]
        save(ngram.Table, file = paste0(path, "filtered_", 1, ".RData"))
        rm(ngram.Table); gc()
        
    ## Filter Chunk 2 of 5
        load(paste0(path, "ngram.Table", 2, ".RData"))
        ngram.Table <- ngram.Table[, list(phrase = sub("_[^_]+$", "", nGram), 
                                          prediction = sub("^([^_]+_)+", "", nGram), count)]
        ngram.Table <- ngram.Table[!(prediction %in% nasty)]
        ngram.Table <- ngram.Table[, phrase.count := sum(count), phrase]
        ngram.Table <- ngram.Table[phrase.count > 2]
        save(ngram.Table, file = paste0(path, "filtered_", 2, ".RData"))
        rm(ngram.Table); gc()
        
    ## Filter Chunk 3 of 5
        load(paste0(path, "ngram.Table", 3, ".RData"))
        ngram.Table <- ngram.Table[, list(phrase = sub("_[^_]+$", "", nGram), 
                                          prediction = sub("^([^_]+_)+", "", nGram), count)]
        ngram.Table <- ngram.Table[!(prediction %in% nasty)]
        ngram.Table <- ngram.Table[, phrase.count := sum(count), phrase]
        ngram.Table <- ngram.Table[phrase.count > 2]
        save(ngram.Table, file = paste0(path, "filtered_", 3, ".RData"))
        rm(ngram.Table); gc()
        
    ## Filter Chunk 4 of 5
        load(paste0(path, "ngram.Table", 4, ".RData"))
        ngram.Table <- ngram.Table[, list(phrase = sub("_[^_]+$", "", nGram), 
                                          prediction = sub("^([^_]+_)+", "", nGram), count)]
        ngram.Table <- ngram.Table[!(prediction %in% nasty)]
        ngram.Table <- ngram.Table[, phrase.count := sum(count), phrase]
        ngram.Table <- ngram.Table[phrase.count > 2]
        save(ngram.Table, file = paste0(path, "filtered_", 4, ".RData"))
        rm(ngram.Table); gc()
        
    ## Filter Chunk 5 of 5
        load(paste0(path, "ngram.Table", 5, ".RData"))
        ngram.Table <- ngram.Table[, list(phrase = sub("_[^_]+$", "", nGram), 
                                          prediction = sub("^([^_]+_)+", "", nGram), count)]
        ngram.Table <- ngram.Table[!(prediction %in% nasty)]
        ngram.Table <- ngram.Table[, phrase.count := sum(count), phrase]
        ngram.Table <- ngram.Table[phrase.count > 2]
        save(ngram.Table, file = paste0(path, "filtered_", 5, ".RData"))
        rm(ngram.Table); gc()
        
        rm(nasty)
```

## 6. Concatenate Tables
```{r Concatenate, echo=TRUE}
    load(paste0(path, "filtered_", 1, ".RData"))
    ngram.all <- ngram.Table
    load(paste0(path, "filtered_", 2, ".RData"))
    ngram.all <- rbind(ngram.all, ngram.Table)
    load(paste0(path, "filtered_", 3, ".RData"))
    ngram.all <- rbind(ngram.all, ngram.Table)
    load(paste0(path, "filtered_", 4, ".RData"))
    ngram.all <- rbind(ngram.all, ngram.Table)
    load(paste0(path, "filtered_", 5, ".RData"))
    ngram.all <- rbind(ngram.all, ngram.Table)
    ngram.Table <- ngram.all
    rm(ngram.all)
    save(ngram.Table, file = paste0(path, "filtered.RData"))
    
    rm(ngram.Table)
```

## 7. Process any aggregates, calculate probabilities, and save as text file
    Load the saved filtered data
``` {r load, echo=TRUE}
    load(paste0(path, "filtered.RData"))
```
    Sum the phrases and predictions in the table, then sort by phrase
``` {r sort, echo=TRUE}
    ngram.Table <- ngram.Table[, lapply(.SD, sum, na.rm = T), by = list(phrase, prediction)]
    ngram.Table <- setorder(ngram.Table, phrase, -count)[, index := seq_len(.N), by = phrase]
```
    Set the number of predictions to be returned, then calculate probabilites for each prediction following a phrase
```{r probability, echo=TRUE}
    ngram.Table <- ngram.Table[index <= 8] 
    ngram.Table[, probability := count / phrase.count]
    ngram.Table <- setorder(ngram.Table, phrase, -probability)[, index := seq_len(.N), by = phrase]
```
    Write to file, named 'filtered.txt', which will be sourced and read by the Prognosticator Function
```{r write, echo=TRUE}
    fwrite(ngram.Table[, c("phrase", "prediction", "probability")], 
           file = paste0(path, "filtered.txt"), append = F)
    rm(ngram.Table); gc()
```
    
# Execute
## 8. Source 'Prognosticator.R'
```{r Prognosticator}
    source("Prognosticator.R")
```
